{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Knowledge Graphs and Deep Learning techniques for Categorizing Tweets\n",
    "## BERT Models (TweetBERT, BERT, RoBERTa, CamemBERT, DistilBERT, Albert, Flaubert)\n",
    "\n",
    "\n",
    "Authors:\n",
    "\n",
    "\n",
    "Experiments:\n",
    "* Applying RF, RNN and Bi-LSTM models to 2 datasets for classifying 4 binary categories.\n",
    "* 2 datatasets: (i) textual information and (ii) textual information and embeddings obtained from knowledge graph exploitation (KGE).\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaben\\Anaconda3\\envs\\tib1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "from ast import literal_eval\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'entities_instances_wikidata':literal_eval,\n",
    "                            'spacy_entities_ids':literal_eval,\n",
    "                            'spacy_entities_labels':literal_eval,\n",
    "                            'falcon_spacy_entities':literal_eval,\n",
    "                            'falcon_spacy_labels':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd2_mw100_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval,\n",
    "                            'sent_embedding_2':literal_eval},error_bad_lines=False)\n",
    "\n",
    "'''\n",
    "tweets = pd.read_csv('dis-dataset-falcon_spacy2-embeddings-sentence-md4.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                           'sent_embedding_1':literal_eval},error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_words = 10000 # cut texts after this number of words\n",
    "maxlen = 20\n",
    "batch_size = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                           deeds reason may allah forgive us\n",
      "1                                                       forest fire near la ronge sask canada\n",
      "2    residents asked shelter place notified officers evacuation shelter place orders expected\n",
      "3                                           13000 people receive evacuation orders california\n",
      "4                                                      got sent photo ruby smoke pours school\n",
      "Name: text_cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets1 = tweets.copy()\n",
    "tweets1['text_cleaned'] = tweets['text'].apply(lambda s : clean_tweet(s))\n",
    "print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features ...\n",
      "  DONE.\n",
      "Dataset contains 7,309 samples.\n"
     ]
    }
   ],
   "source": [
    "# This will hold all of the dataset samples, as strings.\n",
    "sen_w_feats = []\n",
    "\n",
    "# The labels for the samples.\n",
    "labels = []\n",
    "\n",
    "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
    "data_df = tweets.copy()\n",
    "\n",
    "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
    "# replace the NaN values with empty string.\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "# Combining features following https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
    "print('Combining features ...')\n",
    "\n",
    "# For each of the samples...\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # Piece it together...    \n",
    "    combined = row[\"text\"]\n",
    "    combined += \" {:} \".format(row[\"sent_embedding_1\"])\n",
    "    \n",
    "    # Add the combined text to the list.\n",
    "    sen_w_feats.append(combined)\n",
    "\n",
    "    # Also record the sample's label.\n",
    "    labels.append(row[\"target\"])\n",
    "\n",
    "print('  DONE.')\n",
    "\n",
    "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sen_w_feats\n",
    "X2 = tweets1['text_cleaned']\n",
    "\n",
    "df = tweets1.copy()\n",
    "Y1 = df['target']\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.562244\n",
       "1.0    0.437756\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args ={\"reprocess_input_data\": True,\n",
    "           \"fp16\":False,\n",
    "             \"evaluate_during_training\": False,\n",
    "             \"evaluate_during_training_verbose\":False,\n",
    "             \"learning_rate\":2e-5,\n",
    "             \"train_batch_size\":4,\n",
    "             \"eval_batch_size\":4,\n",
    "           \"num_train_epochs\": 10, 'overwrite_output_dir': True, \"evaluation_strategy\":'epochs'\n",
    "            }\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "\n",
    "def calcule_f1(df):\n",
    "    return(df['tp'] / (df['tp'] + 0.5 * (df['fp'] + df['fn'])))\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "dfEval1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BERT Models applied to Category I - Tweets written by people suffering Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\jaben\\Anaconda3\\envs\\tib1\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:586: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfe4ec5b62b4f3fa89d645f4822b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de98246daaa347349d723825353a14fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ba93cf9ef3448880c480997f471c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d2a1fc5f21401ea0c5d05311c9af08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6318bbf7b54e07b5df52989fce8fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d54ba8b15242a5af4c4b1f73ee0ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2e2ade198b4e67af8b0f55d75ff705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f137d8297d43b39e52540ce41bc33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/1279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "limitsave=0\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "train_df1 = pd.DataFrame({ 'text_cleaned': X1_train, 'target': y1_train })\n",
    "test_df1 = pd.DataFrame({ 'text_cleaned': X1_test, 'target': y1_test })\n",
    "\n",
    "c_model_1 = [\"bertweet\",\"bert\",\"roberta\", \"distilbert\",\"camembert\",  \"albert\", \"flaubert\"]\n",
    "c_model_2 = [\"vinai/bertweet-base\",\"bert-base-multilingual-cased\",\"roberta-base\",\"distilbert-base-cased\", \"camembert-base\", \"albert-base-v1\", \"flaubert/flaubert_base_cased\"]\n",
    "\n",
    "for idx, model in enumerate(c_model_1):\n",
    "    \n",
    "    for i in range(0,N_ITER):\n",
    "        \n",
    "        model1 = ClassificationModel(\n",
    "        c_model_1[idx], c_model_2[idx],\n",
    "            use_cuda = True,\n",
    "            args=train_args\n",
    "        )\n",
    "        model1.train_model(train_df1)\n",
    "        result1, model_outputs1, wrong_predictions1 = model1.eval_model(test_df1, f1=f1_multiclass, acc=accuracy_score)\n",
    "        print(result1)\n",
    "        \n",
    "        if(i<limitsave):\n",
    "            torch.save(model1, 'model1'+str(i)+'.pt')\n",
    "        del model1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if(i==0):\n",
    "            dfResultsModels1 = pd.DataFrame.from_dict(result1, orient=\"index\").T\n",
    "        else:\n",
    "            dfResultsModels1b = pd.DataFrame.from_dict(result1, orient=\"index\").T\n",
    "            dfResultsModels1 = dfResultsModels1.append(dfResultsModels1b)\n",
    "\n",
    "    dfResultsModels1Trans = pd.DataFrame(dfResultsModels1.mean(axis=0)).T\n",
    "    dfResultsModels1Trans['f1'] = calcule_f1(dfResultsModels1Trans)\n",
    "    if(idx == 0):\n",
    "        dfResultsModelsTotal = dfResultsModels1Trans.copy()\n",
    "    else:\n",
    "        dfResultsModelsTotal = dfResultsModelsTotal.append(dfResultsModels1Trans)\n",
    "    dfResultsModelsTotal.to_csv('dfResultsModelsTotalEMB-1.csv')\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X2, Y1, test_size=0.3, random_state=42)\n",
    "train_df1 = pd.DataFrame({ 'text_cleaned': X1_train, 'target': y1_train })\n",
    "test_df1 = pd.DataFrame({ 'text_cleaned': X1_test, 'target': y1_test })\n",
    "for idx, model in enumerate(c_model_1):\n",
    "    \n",
    "    for i in range(0,N_ITER):\n",
    "        model1 = ClassificationModel(\n",
    "        c_model_1[idx], c_model_2[idx],\n",
    "            use_cuda = True,\n",
    "            args=train_args\n",
    "        )\n",
    "        model1.train_model(train_df1)\n",
    "        result1, model_outputs1, wrong_predictions1 = model1.eval_model(test_df1, f1=f1_multiclass, acc=accuracy_score)\n",
    "        print(result1)\n",
    "        if(i<limitsave):\n",
    "            torch.save(model1, 'model1'+str(i)+'.pt')\n",
    "        del model1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if(i==0):\n",
    "            dfResultsModels1w = pd.DataFrame.from_dict(result1, orient=\"index\").T\n",
    "        else:\n",
    "            dfResultsModels1bw = pd.DataFrame.from_dict(result1, orient=\"index\").T\n",
    "            dfResultsModels1w = dfResultsModels1.append(dfResultsModels1bw)\n",
    "\n",
    "    dfResultsModels1Transw = pd.DataFrame(dfResultsModels1w.mean(axis=0)).T\n",
    "    dfResultsModels1Transw['f1'] = calcule_f1(dfResultsModels1Transw)\n",
    "\n",
    "    if(idx == 0):\n",
    "        dfResultsModelsTotalw = dfResultsModels1Transw.copy()\n",
    "    else:\n",
    "        dfResultsModelsTotalw = dfResultsModelsTotalw.append(dfResultsModels1Transw)\n",
    "    dfResultsModelsTotalw.to_csv('dfResultsModelsTotalEMB-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Results using original dataset (texts) + embeddings obtained from knowledge graph exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexBERT = ['TweetBERT','BERT','RoBERTa','DistilBERT','CamemBERT','Albert','Flaubert']\n",
    "\n",
    "#dfResultsModelsTotal.reindex(indexBERT)\n",
    "dfResultsModelsTotal = dfResultsModelsTotal.reset_index(drop=True)\n",
    "dfResultsModelsTotal.index = indexBERT\n",
    "dfResultsModelsTotal\n",
    "\n",
    "dfResultssModelsTotal1 = dfResultsModelsTotal.copy()\n",
    "dfResultssModelsTotal1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Results using original dataset (texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexBERT = ['TweetBERT','BERT','RoBERTa','DistilBERT','CamemBERT','Albert','Flaubert']\n",
    "\n",
    "#dfResultsModelsTotal.reindex(indexBERT)\n",
    "dfResultsModelsTotalw = dfResultsModelsTotalw.reset_index(drop=True)\n",
    "dfResultsModelsTotalw.index = indexBERT\n",
    "dfResultsModelsTotalw\n",
    "\n",
    "dfResultssModelsTotal1w = dfResultsModelsTotalw.copy()\n",
    "dfResultssModelsTotal1w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Models applied to Category II - Tweets promoting Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT Models applied to Category III - Informative tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Results using original dataset (texts) + embeddings obtained from knowledge graph exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results using original dataset (texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT Models applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Results using original dataset (texts) + embeddings obtained from knowledge graph exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Results using original dataset (texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
