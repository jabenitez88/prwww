{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Knowledge Graphs and Deep Learning techniques for Categorizing Tweets\n",
    "## Random Forest vs RNN vs Bi-LSTM\n",
    "\n",
    "\n",
    "Authors:\n",
    "<!-- ¡ -->\n",
    "\n",
    "Experiments:\n",
    "* Applying RF, RNN and Bi-LSTM models to 2 datasets for classifying 4 binary categories.\n",
    "* 2 datatasets: (i) textual information and (ii) textual information and embeddings obtained from knowledge graph exploitation (KGE).\n",
    " \n",
    " \n",
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICROSOFT\\Anaconda3\\envs\\tib1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "from ast import literal_eval\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'entities_instances_wikidata':literal_eval,\n",
    "                            'spacy_entities_ids':literal_eval,\n",
    "                            'spacy_entities_labels':literal_eval,\n",
    "                            'falcon_spacy_entities':literal_eval,\n",
    "                            'falcon_spacy_labels':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd2_mw100_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval,\n",
    "                            'sent_embedding_2':literal_eval},error_bad_lines=False)\n",
    "\n",
    "'''\n",
    "tweets = pd.read_csv('dis-dataset-falcon_spacy2-embeddings-sentence-md4.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                           'sent_embedding_1':literal_eval},error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import io\n",
    "\n",
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tid\ttext_orig\tED_Patient\tProED\tinformative\tscientific\thashtags\t\n",
    "# entities_instances_wikidata\tspacy_entities_ids\tspacy_entities_labels\t\n",
    "# falcon_spacy_entities\tfalcon_spacy_labels\tfalcon_spacy_embeddingsmd2_mw50_RW\t\n",
    "# falcon_spacy_embeddingsmd2_mw100_RW\tsent_embedding_1\tsent_embedding_2\n",
    "\n",
    "cols = ['id','text_orig','ED_Patient','ProED','informative','scientific','hashtag','entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels','falcon_spacy_entities'\n",
    "       ,'falcon_spacy_labels','falcon_spacy_embeddingsmd2_mw50_RW','falcon_spacy_embeddingsmd2_mw100_RW','sent_embedding_1','sent_embedding_2']\n",
    "\n",
    "\n",
    "cols = ['id','keyword','location','text','target','falcon2_wd_entities_labels','falcon2_wd_entities_ids','falcon2_wd_relations_ids','falcon2_wd_relations_labels','spacy_wd_entities_ids','spacy_wd_entities_urls','spacy_wd_entities_labels','falcon2_wd_entities_ids_cleaned','falcon2_wd_entities_labels_cleaned','falcon_spacy_embeddingsmd4_mw50_RW','sent_embedding_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = tweets.copy()\n",
    "# .drop(['entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels'], axis=1)\n",
    "\n",
    "tweets1['text_cleaned'] = tweets['text'].apply(lambda s : clean_tweet(s))\n",
    "#print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features ...\n",
      "  DONE.\n",
      "Dataset contains 7,309 samples.\n"
     ]
    }
   ],
   "source": [
    "# This will hold all of the dataset samples, as strings.\n",
    "sen_w_feats = []\n",
    "\n",
    "# The labels for the samples.\n",
    "labels = []\n",
    "\n",
    "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
    "data_df = tweets.copy()\n",
    "\n",
    "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
    "# replace the NaN values with empty string.\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "# Combining features following https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
    "print('Combining features ...')\n",
    "\n",
    "# For each of the samples...\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # Piece it together...    \n",
    "    combined = \" {:} \".format(row[\"sent_embedding_1\"])\n",
    "    \n",
    "    # Add the combined text to the list.\n",
    "    sen_w_feats.append(combined)\n",
    "\n",
    "    # Also record the sample's label.\n",
    "    labels.append(row[\"target\"])\n",
    "\n",
    "print('  DONE.')\n",
    "\n",
    "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data to train and test RF models using original dataset and (original dataset+KGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MICROSOFT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "df = tweets1.copy()\n",
    "X = df['text_cleaned']\n",
    "Xc = sen_w_feats\n",
    "\n",
    "#X = np.array(df['falcon_spacy_labels'].tolist())\n",
    "\n",
    "Y1 = df['target']\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "documents2 = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(Xc)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(Xc[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents2.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Xc is Dataset + KGE information\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "Xc = vectorizer.fit_transform(documents2).toarray()\n",
    "tfidfconverter = TfidfTransformer()\n",
    "Xc = tfidfconverter.fit_transform(Xc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RF Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1232    1]\n",
      " [ 919   41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      1.00      0.73      1233\n",
      "         1.0       0.98      0.04      0.08       960\n",
      "\n",
      "    accuracy                           0.58      2193\n",
      "   macro avg       0.77      0.52      0.40      2193\n",
      "weighted avg       0.75      0.58      0.45      2193\n",
      "\n",
      "0.5804833561331509\n",
      "[[1019  214]\n",
      " [ 289  671]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.80      1233\n",
      "         1.0       0.76      0.70      0.73       960\n",
      "\n",
      "    accuracy                           0.77      2193\n",
      "   macro avg       0.77      0.76      0.76      2193\n",
      "weighted avg       0.77      0.77      0.77      2193\n",
      "\n",
      "0.7706338349293206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "\n",
    "X_orig = df['text_cleaned']\n",
    "X_b = np.array(df['sent_embedding_1'].tolist())\n",
    "X_b = np.asarray(X_b, dtype=np.float32)\n",
    "\n",
    "Y1 = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 100\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_orig)\n",
    "sequences = tok.texts_to_sequences(X_orig)\n",
    "X_c = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7309, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7309, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1000\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "'''y1_pred = model.predict(X1_test)\n",
    "y1_pred = np.argmax(y1_pred, axis=1)\n",
    "conf_mat = confusion_matrix(y1_test, y1_pred)'''\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. RNN Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6880 - acc: 0.5512 - f1_m: 0.0223 - precision_m: 0.0210 - recall_m: 0.0363WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.6880 - acc: 0.5512 - f1_m: 0.0223 - precision_m: 0.0210 - recall_m: 0.0363 - val_loss: 0.6861 - val_acc: 0.5596 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 2s 79ms/step - loss: 0.6873 - acc: 0.5579 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6863 - val_acc: 0.5596 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.686\n",
      "  Accuracy: 0.562\n",
      " fº 0.000\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.6316 - acc: 0.6467 - f1_m: 0.3731 - precision_m: 0.6244 - recall_m: 0.3237 - val_loss: 0.5415 - val_acc: 0.7414 - val_f1_m: 0.6673 - val_precision_m: 0.7663 - val_recall_m: 0.5932\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 5s 182ms/step - loss: 0.4511 - acc: 0.7922 - f1_m: 0.7482 - precision_m: 0.8020 - recall_m: 0.7104 - val_loss: 0.5337 - val_acc: 0.7505 - val_f1_m: 0.6445 - val_precision_m: 0.8553 - val_recall_m: 0.5189\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 5s 190ms/step - loss: 0.4023 - acc: 0.8185 - f1_m: 0.7791 - precision_m: 0.8358 - recall_m: 0.7386 - val_loss: 0.5113 - val_acc: 0.7674 - val_f1_m: 0.7266 - val_precision_m: 0.7482 - val_recall_m: 0.7079\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 0.3725 - acc: 0.8380 - f1_m: 0.8056 - precision_m: 0.8489 - recall_m: 0.7714 - val_loss: 0.5269 - val_acc: 0.7681 - val_f1_m: 0.7110 - val_precision_m: 0.7821 - val_recall_m: 0.6536\n",
      "Test set\n",
      "  Loss: 0.482\n",
      "  Accuracy: 0.782\n",
      " fº 0.723\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. RNN Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Bi-LSTM Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "160/160 [==============================] - 9s 55ms/step - loss: 0.6874 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6867 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6856 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "160/160 [==============================] - 6s 41ms/step - loss: 0.6865 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6856 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6865 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/14\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6865 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6855 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/14\n",
      "160/160 [==============================] - 6s 41ms/step - loss: 0.6865 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6856 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/14\n",
      "160/160 [==============================] - 6s 41ms/step - loss: 0.6864 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6864 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6855 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6854 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/14\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.6866 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6857 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "69/69 [==============================] - 1s 15ms/step - loss: 0.6857 - acc: 0.5622 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.686\n",
      "  Accuracy: 0.562\n",
      " fº 0.000\n",
      "Epoch 1/14\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.6805 - acc: 0.5584 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6596 - val_acc: 0.5622 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.5548 - acc: 0.6880 - f1_m: 0.4368 - precision_m: 0.6904 - recall_m: 0.3472 - val_loss: 0.4838 - val_acc: 0.7620 - val_f1_m: 0.6437 - val_precision_m: 0.9260 - val_recall_m: 0.4976\n",
      "Epoch 3/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.4466 - acc: 0.7936 - f1_m: 0.7147 - precision_m: 0.8821 - recall_m: 0.6151 - val_loss: 0.4702 - val_acc: 0.7893 - val_f1_m: 0.7167 - val_precision_m: 0.8656 - val_recall_m: 0.6160\n",
      "Epoch 4/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.4121 - acc: 0.8124 - f1_m: 0.7453 - precision_m: 0.8926 - recall_m: 0.6531 - val_loss: 0.4743 - val_acc: 0.7852 - val_f1_m: 0.7108 - val_precision_m: 0.8572 - val_recall_m: 0.6122\n",
      "Epoch 5/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3953 - acc: 0.8231 - f1_m: 0.7644 - precision_m: 0.8978 - recall_m: 0.6791 - val_loss: 0.4755 - val_acc: 0.7798 - val_f1_m: 0.7031 - val_precision_m: 0.8529 - val_recall_m: 0.6038\n",
      "Epoch 6/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3807 - acc: 0.8317 - f1_m: 0.7768 - precision_m: 0.9139 - recall_m: 0.6856 - val_loss: 0.4881 - val_acc: 0.7766 - val_f1_m: 0.6957 - val_precision_m: 0.8560 - val_recall_m: 0.5911\n",
      "Epoch 7/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3705 - acc: 0.8405 - f1_m: 0.7872 - precision_m: 0.9138 - recall_m: 0.7037 - val_loss: 0.4974 - val_acc: 0.7770 - val_f1_m: 0.7104 - val_precision_m: 0.8192 - val_recall_m: 0.6325\n",
      "Epoch 8/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3633 - acc: 0.8470 - f1_m: 0.8005 - precision_m: 0.9173 - recall_m: 0.7199 - val_loss: 0.5134 - val_acc: 0.7743 - val_f1_m: 0.7060 - val_precision_m: 0.8159 - val_recall_m: 0.6279\n",
      "Epoch 9/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3574 - acc: 0.8495 - f1_m: 0.8034 - precision_m: 0.9147 - recall_m: 0.7298 - val_loss: 0.5128 - val_acc: 0.7684 - val_f1_m: 0.6855 - val_precision_m: 0.8385 - val_recall_m: 0.5849\n",
      "Epoch 10/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3491 - acc: 0.8501 - f1_m: 0.8030 - precision_m: 0.9137 - recall_m: 0.7286 - val_loss: 0.5206 - val_acc: 0.7747 - val_f1_m: 0.7075 - val_precision_m: 0.8149 - val_recall_m: 0.6312\n",
      "Epoch 11/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3410 - acc: 0.8565 - f1_m: 0.8139 - precision_m: 0.9210 - recall_m: 0.7368 - val_loss: 0.5266 - val_acc: 0.7715 - val_f1_m: 0.6973 - val_precision_m: 0.8204 - val_recall_m: 0.6119\n",
      "Epoch 12/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3343 - acc: 0.8630 - f1_m: 0.8222 - precision_m: 0.9293 - recall_m: 0.7494 - val_loss: 0.5447 - val_acc: 0.7715 - val_f1_m: 0.7082 - val_precision_m: 0.7988 - val_recall_m: 0.6430\n",
      "Epoch 13/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3195 - acc: 0.8657 - f1_m: 0.8197 - precision_m: 0.9236 - recall_m: 0.7462 - val_loss: 0.6086 - val_acc: 0.7633 - val_f1_m: 0.7076 - val_precision_m: 0.7678 - val_recall_m: 0.6641\n",
      "Epoch 14/14\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.3122 - acc: 0.8704 - f1_m: 0.8319 - precision_m: 0.9301 - recall_m: 0.7614 - val_loss: 0.6203 - val_acc: 0.7597 - val_f1_m: 0.7076 - val_precision_m: 0.7548 - val_recall_m: 0.6735\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.6203 - acc: 0.7597 - f1_m: 0.7026 - precision_m: 0.7557 - recall_m: 0.6748\n",
      "Test set\n",
      "  Loss: 0.620\n",
      "  Accuracy: 0.760\n",
      " fº 0.703\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
